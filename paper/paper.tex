\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{unicode-math}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

\title{An analysis of GNN and graph representational models on real-world datasets}

\author{
\IEEEauthorblockN{Spiros Maggioros\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}
\IEEEauthorblockA{
Athens, Greece \\
Spiros.Maggioros@pennmedicine.upenn.edu
}
\and
\IEEEauthorblockN{Rei Pasai\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
Athens, Greece \\
reidipasai03@gmail.com
}
\linebreakand
\IEEEauthorblockN{Eleni Nasopoulou\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
Athens, Greece \\
el21087@mail.ntua.gr
}
\\
\IEEEauthorblockA{
\IEEEauthorrefmark{1}National Technical University of Athens, Athens, Greece \\
\IEEEauthorrefmark{2}AI$^2$D Center, University of Pennsylvania, Philadelphia, PA, USA
}
}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
Graphs, Graph Neural Networks, Kernels, Deep Learning, Representation Learning
\end{IEEEkeywords}

\section{Introduction}

Graph-structured data appear across diverse domains from molecular chemistry and protein networks to social media and cybersecurity systems. The fundamental challenge lies in transforming graph structures into fixed-dimensional vectors amenable to machine learning algorithms. Graph embedding methods address this by mapping graphs into continuous vector spaces that preserve structural and semantic properties while enabling efficient computation. Traditional graph analysis relies on graph kernels that use handcrafted features such as shortest paths, graphlets, and random walks \cite{shervashidze2011weisfeiler}. While theoretically sound, these approaches suffer from poor generalization across domains, require domain expertise for feature engineering, and fail to capture complex patterns in large-scale networks. Moreover, kernel methods typically lack explicit embeddings, restricting compatibility with general-purpose learning algorithms. Recent advances in deep learning have enabled a shift toward data-driven graph embeddings. Inspired by natural language processing techniques that learn distributed representations of words and documents \cite{mikolov2013efficientestimationwordrepresentations, mikolov2013distributedrepresentationswordsphrases}, \textbf{(change modern)} modern approaches treat graphs as documents and subgraphs as words. This paradigm encompasses unsupervised methods like Graph2Vec \cite{narayanan2017graph2veclearningdistributedrepresentations} and DeepWalk \cite{Perozzi_2014} that learn task-agnostic representations.

Graph2Vec, DeepWalk, and NetLSD \cite{Tsitsulin_2018} are unsupervised methods that learn exclusively from graph topology without node features or labels. This structural focus enables rapid training and high computational efficiency, making them suitable for large-scale applications where labeled data is unavailable. However, ignoring node attributes typically yields lower classification accuracy than supervised methods. These embeddings often serve as auxiliary features for node-level prediction or initialization for supervised models. This work evaluates these trade-offs through empirical benchmarking across classification, clustering, efficiency, and robustness.

First introduced by Micheli, 2009 \cite{MicheliGNN} and Scarselli et al., 2008 \cite{scarselliGNN} as a form of recurrent neural network \cite{kipf2017semisupervisedclassificationgraphconvolutional}, Graph Neural Networks (GNNs) have recently become a widely used method to tackle problems like classification and link prediction tasks in domains like social networks, telecommunication networks, biological networks or brain connectomes \cite{veličković2018graphattentionnetworks} where other learning techniques fail. Almost all modern message-passing GNN models use convolutional or attentional layers and an aggregation process to compute latent embeddings. Thus, GNNs excel when applied to networks where nodes include features, in contrast with graph representation learning models that only capture the network structure. Despite their current appeal, GNNs are criticized by many for their limited expressiveness and generalization. Xu et al. \cite{xu2019powerfulgraphneuralnetworks} proved that GNNs are as powerful as the 1-WL algorithm and highlighted the minimal discriminative power of commonly used aggregators. Currently, GNN models occupy top positions on the Open Graph Benchmark (OGB) \cite{opengraphbenchmarkdatasets} and Long-Range Graph Benchmark (LRGB) \cite{dwivedi2023longrangegraphbenchmark} leaderboards. Nevertheless, classic GNNs still lead the leaderboard in some of these benchmarks \cite{luo2025classicgnnsstrongbaselines}. In this work, we are going to evaluate a number of these models on social, bio-informatics and small molecules networks evaluating classification performance, clustering quality, computational efficiency, and robustness to perturbations. Our contributions include comprehensive empirical comparison under controlled conditions, investigation of embedding dimensionality trade-offs, and assessment of robustness characteristics for real-world deployment.

\section{Related Work}

\subsection{Graph2Vec}

Graph2Vec \cite{narayanan2017graph2veclearningdistributedrepresentations} addresses fundamental limitations of node level embedding aggregation by extending document embedding approaches, particularly doc2vec \cite{le2014distributedrepresentationssentencesdocuments, mikolov2013efficientestimationwordrepresentations}, to whole-graph representation learning. The method treats entire graphs as documents and rooted subgraphs extracted through Weisfeiler Lehman relabeling \cite{shervashidze2011weisfeiler} as vocabulary words, enabling unsupervised embedding via skip-gram training \cite{mikolov2013efficientestimationwordrepresentations}. This approach represents a significant departure from traditional graph kernels that rely on handcrafted structural features, instead learning data-driven representations adapted to specific graph corpora. On benchmark graph classification datasets, Graph2Vec achieves $83.15\%$ accuracy on MUTAG, $60.17\%$ on PTC, and $73.30\%$ on PROTEINS, substantially outperforming node-level aggregation approaches by over 10 percentage points \cite{narayanan2017graph2veclearningdistributedrepresentations}. The method demonstrates particular strength on large-scale datasets where data-driven learning shows clear advantages. On Android malware detection involving 10,560 API dependency graphs with average size of 2,637 nodes, Graph2Vec achieves $99.03\%$ accuracy, surpassing Deep WL kernels ($98.16\%$) \cite{yanardag2015deepgraphkernels}, WL kernels ($97.12\%$) \cite{shervashidze2011weisfeiler}, and substantially outperforming node2vec ($81.25\%$) and sub2vec ($76.83\%$) \cite{narayanan2017graph2veclearningdistributedrepresentations, adhikari2017distributedrepresentationsubgraphs}. This 18 percentage point improvement over node2vec aggregation suggests that global structural properties cannot be adequately recovered through local node representations alone.

However, Graph2Vec exhibits mixed performance across different dataset characteristics. On NCI1 and NCI109 molecular datasets, it achieves $73.22\%$ and $74.26\%$ accuracy respectively, compared to approximately $80\%$ for WL kernels \cite{shervashidze2011weisfeiler}. This performance gap suggests that Graph2Vec’s data-driven approach requires sufficient training examples to surpass carefully designed handcrafted features, particularly on smaller datasets with limited structural diversity. Computational efficiency analysis reveals Graph2Vec occupies a middle ground: slower than node2vec due to subgraph extraction overhead, yet significantly faster than sub2vec which requires extensive random walk sampling. The method generalizes well to unseen graphs when trained on large, diverse datasets but struggles with limited training data or low structural variation.

\subsection{DeepWalk}

DeepWalk \cite{Perozzi_2014} pioneered the application of neural language modeling to network representation learning by establishing a formal connection between graph topology and natural language structure. The method treats truncated random walks on graphs as sentences in a corpus, recognizing that vertex sequences exhibit power-law frequency distributions similar to word distributions in natural language. This insight enables direct application of skip-gram models \cite{mikolov2013efficientestimationwordrepresentations} to learn vertex embeddings that capture neighborhood relationships and community structure.

Originally designed for node-level tasks, DeepWalk demonstrates substantial improvements over spectral clustering methods on social network datasets. DeepWalk achieves up to 10\% higher F1-scores on social networks such as BlogCatalog, Flickr, and YouTube, and in some configurations (e.g., on Flickr) it matches or surpasses baselines while using 60\% less labeled training data \cite{Perozzi_2014}. DeepWalk's online learning algorithm and trivial parallelization enable exceptional scalability, processing networks with millions of nodes efficiently. This scalability makes DeepWalk applicable to web-scale graphs where batch spectral methods become computationally prohibitive.

DeepWalk established the foundation for subsequent node embedding methods, most notably node2vec \cite{grover2016node2vecscalablefeaturelearning}, which extends DeepWalk with biased random walk strategies controlled by return parameter $p$ and in-out parameter $q$. Node2vec achieves 22.3\% relative improvement over DeepWalk in Macro-F1 scores on BlogCatalog and 21.8\% on Wikipedia by learning appropriate exploration strategies that balance breadth-first and depth-first search \cite{grover2016node2vecscalablefeaturelearning}. For link prediction tasks, node2vec outperforms DeepWalk by up to 3.8\% in AUC scores across multiple network types. However, DeepWalk's uniform random walk strategy provides limited control over neighborhood exploration, potentially missing important structural patterns that biased strategies can capture.

A critical limitation emerges when applying DeepWalk to whole-graph tasks. Since DeepWalk produces node-level embeddings, adapting it for graph-level representation requires aggregation strategies such as averaging or pooling. Simply averaging DeepWalk or node2vec embeddings yields only 81.25\% accuracy on Android malware detection \cite{narayanan2017graph2veclearningdistributedrepresentations}, an 18 percentage point gap compared to methods designed explicitly for graph-level representation. This substantial performance difference demonstrates that global structural properties are not adequately captured through local node aggregation, motivating the development of whole-graph embedding approaches like Graph2Vec that directly learn graph-level representations.

\subsection{Graph Neural Networks}
Due to deep learning being a defacto standard for various tasks, a lot of effort has been put to generalize neural networks to graph structured data. A pioneer modern work was proposed by Kipf and Welling \cite{kipf2017semisupervisedclassificationgraphconvolutional}, GCNs are still used to this day due to their scalability. Another big step was made by veličković et al. with Graph Attentional Networks(GAT) \cite{veličković2018graphattentionnetworks} that leveraged masked self-attentional layers enabling nodes to attend over their neighborhood's features. This was later improved by Brody et al. \cite{brody2022attentivegraphattentionnetworks} proposing GATv2, improving the average error by $11.5\%$ in some tasks. In 2019, Xu et al. \cite{xu2019powerfulgraphneuralnetworks} proposed Graph Isomorphism Networks(GIN), proving that GNNs are at most as powerful as the 1-WL test and identifying graph structures that aforementioned GNNs can't distinguish. Corso et al. \cite{corso2020principalneighbourhoodaggregationgraph}, taking advantage of the analysis presented at \cite{xu2019powerfulgraphneuralnetworks}, introduced Principal Neighborhood Aggregation(PNA), a architecture that combines multiple aggregators with degree-scalers, proving that using a single aggregator fails to distinguish simple graph structures. All Aforementioned GNNs make up the "classic" models, modern GNN models leverage SSL \cite{chien2022nodefeatureextractionselfsupervised}, \cite{veličković2018deepgraphinfomax}, which is particularly useful in graph datasets where we might have text or images as input and labels are not always known. The GIANT framework proposed at \cite{chien2022nodefeatureextractionselfsupervised} uses XR-Transformers, a model that is considered state-of-the-art, the resulting transformer is then used as ans encoder to generate numerical node features for each node, the encoder can change to address other data forms like images, where a CNN can be used. Though the GIANT framework dominates node property prediction leaderboards, it's not widely used for graph-level tasks, and interestingly so, work proposed by \cite{luo2025classicgnnsstrongbaselines} showed that classic GNN models can still produce strong results at graph-level tasks when used with edge features, normalization, dropout, residual connections, feed-forward networks and positional encoding.

\section{General Notation}
We define a Graph as $G(V, E)$ Where $V = \{1, 2, ..., n\}$ represents the nodes and $E = \{ (u_i, v_i), ..., (u_j, v_j)\}$ represents the edges. Each graph can appear as an adjacency list where we map nodes with lists of other nodes or with an adjacency matrix $A \in \{0, 1\}^{n \times n}$ where $A_{ij} = 1: (u_i, u_j) \in E$. We also define a node labeling function $\lambda_i$ that maps nodes to a class $l$ and a graph labeling function $\mu_i$ that maps graphs to a class $c$. We will have two types of graphs in total, graphs that have node attributes, thus, for each node $u_i \in V \longrightarrow h_i \in R^d$ and graphs that don't have node attributes, thus, we have to learn from structure only. Our objective for graph classification tasks is to learn a function that produces optimal embeddings for each graph $f(G, \mu): R^{n \times n} \longrightarrow R^d$ and for node classification we have to learn a function $f$ that produces optimal embeddings for each node $f(G, \lambda): R^{n \times n} \longrightarrow R^{n \times d}$. For short, we define these embeddings as $\Phi$.

\section{Models}

\subsection{Graph Representation Learning Models}

\subsubsection*{\textbf{Graph2Vec}}

Graph2Vec \cite{narayanan2017graph2veclearningdistributedrepresentations} produces embeddings $\Phi \in \mathbb{R}^{G \times d}$ for a dataset of graphs $\mathcal{G} = \{G_1, G_2, ..., G_n\}$.
Rooted subgraph extraction follows the Weisfeiler--Lehman procedure \cite{shervashidze2011weisfeiler}. For a root node $n$ and height $h$, the procedure generates identifier $sg_n^{(h)}$ through iterative relabeling based on sorted multisets of neighbor labels, where height corresponds to the maximum distance from the root node. The training objective for each subgraph $sg_n^{(h)}$ extracted from graph $G_i$ is:
\begin{equation}
J(\Phi) = -\log \Pr(sg_n^{(h)} | \Phi(G_i))
\end{equation}
where $\Phi(G_i) \in \mathbb{R}^{\delta}$ represents the graph embedding vector.

Computing $\Pr(sg_n^{(h)} | \Phi(G_i))$ with softmax normalization over the full subgraph vocabulary requires $O(|V_{sg}|)$ complexity. Graph2Vec instead applies negative sampling from the doc2vec PV-DBOW framework, approximating the objective by sampling $k$ negative subgraphs not present in the target graph. This reduces the per-update cost to $O(k)$ where $k \ll |V_{sg}|$. The gradient update follows:
\begin{equation}
\Phi \leftarrow \Phi - \alpha \frac{\partial J}{\partial \Phi}
\end{equation}
where $\alpha$ denotes the learning rate. Training iterates over all graphs for $e$ epochs, shuffling graphs at each epoch. For each graph, rooted subgraphs up to height $h$ are extracted around every node before applying stochastic gradient descent updates.

\subsubsection*{\textbf{DeepWalk}}

DeepWalk \cite{Perozzi_2014} generates $d$-dimensional node embeddings $\Phi \in \mathbb{R}^{|V| \times d}$ for graph $G = (V, E)$. The algorithm samples $\gamma$ random walks of length $t$ from each vertex $v_i \in V$, with each walk $W_{v_i}$ uniformly sampling neighbors at each step.

The skip-gram training objective \cite{mikolov2013efficientestimationwordrepresentations} processes each vertex $v_j$ in walk $W_{v_i}$ by iterating over a context window of size $w$. For each context vertex $u_k \in W_{v_i}[j-w : j+w]$, the algorithm minimizes:
\begin{equation}
J(\Phi) = -\log \Pr(u_k | \Phi(v_j))
\end{equation}

Direct computation of $\Pr(u_k | \Phi(v_j))$ via softmax over all vertices requires $O(|V|)$ operations per sample. Hierarchical softmax reduces this to $O(\log |V|)$ by arranging vertices as leaves of a binary tree. For vertex $u_k$ with tree path $(b_0, b_1, ..., b_{\lceil \log |V| \rceil})$ from root to leaf:
\begin{equation}
\Pr(u_k | \Phi(v_j)) = \prod_{l=1}^{\lceil \log |V| \rceil} \Pr(b_l | \Phi(v_j))
\end{equation}
where each factor represents a binary classifier at tree node $b_l$. Huffman encoding assigns shorter paths to frequently sampled vertices, further improving efficiency.

The algorithm executes $\gamma$ passes over vertices. The learning rate $\alpha$ begins at $2.5\%$ and decreases linearly with the number of vertices processed \cite{Perozzi_2014}. The power-law distribution of vertex frequencies in random walks creates sparse gradient updates, enabling lock-free asynchronous parallel training across multiple workers.

For graph-level tasks, node embeddings aggregate via mean pooling $\Phi(G) = \frac{1}{|V|}\sum_{v \in V} \Phi(v)$ or max pooling, though such simple aggregation cannot recover global structural properties effectively \cite{narayanan2017graph2veclearningdistributedrepresentations}.

\subsection{Graph Neural Network Models}

\subsubsection*{\textbf{GCN}}

\subsubsection*{\textbf{GAT}}
Graph Attention Network(GAT) \cite{veličković2018graphattentionnetworks} was the first model that introduced masked self-attention to GNN models, enabling each node to assign different importances to other nodes of the same neighborhood. This is also beneficial for interpretability. The model is applied to graphs that contain node attributes $h = \{ h_1, h_2, ..., h_n\}$, $h_i \in R^{d}$, each layer produce new embeddings $h' = \{ h_1', h_2', ..., h_n'\}$, $h_i' \in R^{d'}$. Self-attention is applied to all the nodes with a shared attention mechanism $\alpha: R^d \times R^d \longrightarrow R$ that computes attention coefficients \[e_{ij} = \alpha(Wh_i, Wh_j)\] that indicates importance of node's j features to node i. $W \in R ^{d' \times d}$ is a weight matrix. Authors used a single-layer feedforward neural network as the attention mechanism that computes $\alpha_{ij}$. Also, they found multi-head attention beneficial, thus, computing new embeddings $h_i'$ requires concatenating or averaging all the aggregated features from each head \[h_{ij}' = \oplus\sigma(\sum_{j \in N_i}a_{ij}^kW^kh_j)\]
Where $\sigma$ is the simgoid function and $k$ indicates the $k$-th attention mechanism. In the last layer, concatenation is not possible, using averaging instead.

The model is computationally efficient as it needs $O(|V|dd' + |E|d')$ and individual head's computations can be parallelized.
\subsubsection*{\textbf{GIN}}

\subsubsection*{\textbf{PNA}}

\section{Datasets \& Evaluation}
We performed training and evaluation on multiple graph datasets from TUDataset \cite{morris2020tudatasetcollectionbenchmarkdatasets} and a node regression task with data from NiChart's ISTAGING project(paper still under review at Nature Neuroscience).

\subsection{TUDataset}

We selected 4 datasets from TUDataset list in total: ENZYMES, IMDB-MULTI, MUTAG, PROTEINS and REDDIT-MULTI-5K. Out of them, only ENZYMES has node attributes, thus, can be used for GNN training.

\section{Results \& Discussion}

\bibliographystyle{IEEEtran}
\bibliography{references}
\vspace{12pt}


\end{document}
